{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Msc AI - Sem 1 - Etivity 5\n",
    "\n",
    "Name:Michel Danjou\n",
    "\n",
    "ID:18263461\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Feature Selection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds on top of Lab 4 by introducing feature selection into the process of selecting the best classifier for a binary classification problem.\n",
    "\n",
    "The feature selection method applied here is Recursive Feature Elimination (RFE) as demonstrated in the tutorial at https://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/.\n",
    "\n",
    "In this demonstration we use a modified version of the seeds data set (see https://archive.ics.uci.edu/ml/datasets/seeds), which is the same data set used in Lab 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danjou\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import preprocessing #needed for scaling attributes to the nterval [0,1]\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "  * Repeat the experiment in Etivity5-Feature Selection.ipynb with winequality_red.csv and draw initial conclusions from the results you observe.\n",
    "\n",
    "  * Rubric: None\n",
    "\n",
    "  * Download the Jupyter notebook Etivity5-Feature Selection and the data set seeds_dataset_binary.csv from GitLab and look through the notebook.\n",
    "  \n",
    "  * Download the data set winequality_red.csv from GitLab. This is one of the public UCI datasets.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare the dataset for training and evaluation\n",
    "Feel free to apply any other pre-processing technique at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "      <td>1599.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>8.319637</td>\n",
       "      <td>0.527821</td>\n",
       "      <td>0.270976</td>\n",
       "      <td>2.538806</td>\n",
       "      <td>0.087467</td>\n",
       "      <td>15.874922</td>\n",
       "      <td>46.467792</td>\n",
       "      <td>0.996747</td>\n",
       "      <td>3.311113</td>\n",
       "      <td>0.658149</td>\n",
       "      <td>10.422983</td>\n",
       "      <td>5.636023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.741096</td>\n",
       "      <td>0.179060</td>\n",
       "      <td>0.194801</td>\n",
       "      <td>1.409928</td>\n",
       "      <td>0.047065</td>\n",
       "      <td>10.460157</td>\n",
       "      <td>32.895324</td>\n",
       "      <td>0.001887</td>\n",
       "      <td>0.154386</td>\n",
       "      <td>0.169507</td>\n",
       "      <td>1.065668</td>\n",
       "      <td>0.807569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.600000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.990070</td>\n",
       "      <td>2.740000</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7.100000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.995600</td>\n",
       "      <td>3.210000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.079000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>0.996750</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>2.600000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.997835</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>11.100000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.900000</td>\n",
       "      <td>1.580000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>1.003690</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>14.900000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
       "mean        8.319637          0.527821     0.270976        2.538806   \n",
       "std         1.741096          0.179060     0.194801        1.409928   \n",
       "min         4.600000          0.120000     0.000000        0.900000   \n",
       "25%         7.100000          0.390000     0.090000        1.900000   \n",
       "50%         7.900000          0.520000     0.260000        2.200000   \n",
       "75%         9.200000          0.640000     0.420000        2.600000   \n",
       "max        15.900000          1.580000     1.000000       15.500000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
       "mean      0.087467            15.874922             46.467792     0.996747   \n",
       "std       0.047065            10.460157             32.895324     0.001887   \n",
       "min       0.012000             1.000000              6.000000     0.990070   \n",
       "25%       0.070000             7.000000             22.000000     0.995600   \n",
       "50%       0.079000            14.000000             38.000000     0.996750   \n",
       "75%       0.090000            21.000000             62.000000     0.997835   \n",
       "max       0.611000            72.000000            289.000000     1.003690   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
       "mean      3.311113     0.658149    10.422983     5.636023  \n",
       "std       0.154386     0.169507     1.065668     0.807569  \n",
       "min       2.740000     0.330000     8.400000     3.000000  \n",
       "25%       3.210000     0.550000     9.500000     5.000000  \n",
       "50%       3.310000     0.620000    10.200000     6.000000  \n",
       "75%       3.400000     0.730000    11.100000     6.000000  \n",
       "max       4.010000     2.000000    14.900000     8.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab5_df = pd.read_csv(\"./winequality_red.csv\")\n",
    "lab5_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target attribute\n",
    "target_attribute_name = 'quality'\n",
    "target = lab5_df[target_attribute_name]\n",
    "\n",
    "# predictor attributes\n",
    "predictors = lab5_df.drop(target_attribute_name, axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data set into a training (80%) and test (20%) data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pepare independent stratified data sets for training and test of the final model\n",
    "predictors_train, predictors_test, target_train, target_test = train_test_split(\n",
    "    predictors, target, test_size=0.20, shuffle=True, stratify=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale all predictor values to the range [0, 1] after calling train_test_split() to avoid data snooping\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "predictors_train = min_max_scaler.fit_transform(predictors_train)\n",
    "predictors_test = min_max_scaler.fit_transform(predictors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    681\n",
       "6    638\n",
       "7    199\n",
       "4     53\n",
       "8     18\n",
       "3     10\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Apply RFE with SVM for selecting the best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False False False False False False False  True  True]\n",
      "[5 1 8 4 2 7 3 6 9 1 1]\n"
     ]
    }
   ],
   "source": [
    "# create a base classifier used to evaluate a subset of attributes\n",
    "estimatorSVM = svm.SVR(kernel=\"linear\")\n",
    "selectorSVM = RFE(estimatorSVM, 3)\n",
    "selectorSVM = selectorSVM.fit(predictors_train, target_train)\n",
    "# summarize the selection of the attributes\n",
    "print(selectorSVM.support_)\n",
    "print(selectorSVM.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Apply RFE with Logistic Regression for selecting the best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False False False False  True False False False  True]\n",
      "[6 1 4 9 8 5 1 3 7 2 1]\n"
     ]
    }
   ],
   "source": [
    "# create a base classifier used to evaluate a subset of attributes\n",
    "estimatorLR = LogisticRegression()\n",
    "# create the RFE model and select 3 attributes\n",
    "selectorLR = RFE(estimatorLR, 3)\n",
    "selectorLR = selectorLR.fit(predictors_train, target_train)\n",
    "# summarize the selection of the attributes\n",
    "print(selectorLR.support_)\n",
    "print(selectorLR.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Evaluate on the Test Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the selectors to prepare training data sets only with the selected features\n",
    "\n",
    "__Note:__ The same selectors are applied to the test data set. However, it is important that the test data set was not used by (it's invisible to) the selectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_train_SVMselected = selectorSVM.transform(predictors_train)\n",
    "predictors_test_SVMselected = selectorSVM.transform(predictors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors_train_LRselected = selectorLR.transform(predictors_train)\n",
    "predictors_test_LRselected = selectorLR.transform(predictors_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate SVM classifiers with both the selected features and all features \n",
    "\n",
    "Here we train three models:\n",
    "* model1 - with the features selected by SVM\n",
    "* model2 - with the features selected by Logistic Regression\n",
    "* model3 - with all features (i.e. without feature selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = svm.SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6125"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1 = classifier.fit(predictors_train_SVMselected, target_train)\n",
    "model1.score(predictors_test_SVMselected, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5875"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = classifier.fit(predictors_train_LRselected, target_train)\n",
    "model2.score(predictors_test_LRselected, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.584375"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = classifier.fit(predictors_train, target_train)\n",
    "model3.score(predictors_test, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Conclusion\n",
    "\n",
    "The results above, give evidence that model1 is most accurate.\n",
    "\n",
    "However, when you execute this code again, it is very likely to get different results.\n",
    "\n",
    "To get more accurate results, accounting for the variance in the results, it is better to run the whole experiment multiple times and measure the variance in the results. Then pick the model that gives better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "  * Repeat the experiment in Etivity5-Feature Selection.ipynb with winequality_red.csv but this time run the whole experiment in a loop with different training/test stratified splits. \n",
    "  * Evaluate the three models for each iteration of the loop separately and boxplot the accuracies of the models for each iteration. \n",
    "  * Compare the results of this experiment to the results of Task 1 in a markdown cell (max 150 words).\n",
    "  \n",
    "#### Resources\n",
    "  * https://machinelearningmastery.com/an-introduction-to-feature-selection/\n",
    "  * https://machinelearningmastery.com/feature-selection-in-python-with-scikit-learn/\n",
    "  * https://stats.stackexchange.com/questions/264533/how-should-feature-selection-and-hyperparameter-optimization-be-ordered-in-the-m\n",
    "\n",
    "\n",
    "#### Rubric\n",
    "\n",
    "| Beginning [0-8]       | Developing [9-12]           | Accomplished [13-16]  |Exemplary [17-20] |\n",
    "| ------------- |:-------------:| -----:|-------------|\n",
    "| Loop implemented with run-time errors.      | Loop runs without errors. Feature selection performed outside the loop. | Loop implemented correctly with feature selection performed inside the loop. Results not box-plotted or summarised correctly. |Loop implemented correctly with feature selection performed inside the loop. Results box-plotted and summarised correctly.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Additional information regarding the stratified parameter used by train_test_split\n",
    "https://stackoverflow.com/questions/34842405/parameter-stratify-from-method-train-test-split-scikit-learn\n",
    "\n",
    "This `stratify` parameter makes a split so that the proportion of values in the sample produced will be the same as the proportion of values provided to parameter `stratify`.\n",
    "\n",
    "For example, if variable `y` is a binary categorical variable with values `0` and `1` and there are 25% of zeros and 75% of ones, `stratify=y` will make sure that your random split has 25% of `0's` and 75% of `1's`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment iteration: 0\n",
      "Experiment iteration: 1\n",
      "Experiment iteration: 2\n",
      "Experiment iteration: 3\n",
      "Experiment iteration: 4\n",
      "Experiment iteration: 5\n",
      "Experiment iteration: 6\n",
      "Experiment iteration: 7\n",
      "Experiment iteration: 8\n",
      "Experiment iteration: 9\n",
      "Experiment iteration: 10\n",
      "Experiment iteration: 11\n",
      "Experiment iteration: 12\n",
      "Experiment iteration: 13\n",
      "Experiment iteration: 14\n",
      "Experiment iteration: 15\n",
      "Experiment iteration: 16\n",
      "Experiment iteration: 17\n",
      "Experiment iteration: 18\n",
      "Experiment iteration: 19\n",
      "Experiment iteration: 20\n",
      "Experiment iteration: 21\n",
      "Experiment iteration: 22\n",
      "Experiment iteration: 23\n",
      "Experiment iteration: 24\n",
      "Experiment iteration: 25\n",
      "Experiment iteration: 26\n",
      "Experiment iteration: 27\n",
      "Experiment iteration: 28\n",
      "Experiment iteration: 29\n",
      "Experiment iteration: 30\n",
      "Experiment iteration: 31\n",
      "Experiment iteration: 32\n",
      "Experiment iteration: 33\n",
      "Experiment iteration: 34\n",
      "Experiment iteration: 35\n",
      "Experiment iteration: 36\n",
      "Experiment iteration: 37\n",
      "Experiment iteration: 38\n",
      "Experiment iteration: 39\n",
      "Experiment iteration: 40\n",
      "Experiment iteration: 41\n",
      "Experiment iteration: 42\n",
      "Experiment iteration: 43\n",
      "Experiment iteration: 44\n",
      "Experiment iteration: 45\n",
      "Experiment iteration: 46\n",
      "Experiment iteration: 47\n",
      "Experiment iteration: 48\n",
      "Experiment iteration: 49\n",
      "Experiment iteration: 50\n",
      "Experiment iteration: 51\n",
      "Experiment iteration: 52\n",
      "Experiment iteration: 53\n",
      "Experiment iteration: 54\n",
      "Experiment iteration: 55\n",
      "Experiment iteration: 56\n",
      "Experiment iteration: 57\n",
      "Experiment iteration: 58\n",
      "Experiment iteration: 59\n",
      "Experiment iteration: 60\n",
      "Experiment iteration: 61\n",
      "Experiment iteration: 62\n",
      "Experiment iteration: 63\n",
      "Experiment iteration: 64\n",
      "Experiment iteration: 65\n",
      "Experiment iteration: 66\n",
      "Experiment iteration: 67\n",
      "Experiment iteration: 68\n",
      "Experiment iteration: 69\n",
      "Experiment iteration: 70\n",
      "Experiment iteration: 71\n",
      "Experiment iteration: 72\n",
      "Experiment iteration: 73\n",
      "Experiment iteration: 74\n",
      "Experiment iteration: 75\n",
      "Experiment iteration: 76\n",
      "Experiment iteration: 77\n",
      "Experiment iteration: 78\n",
      "Experiment iteration: 79\n",
      "Experiment iteration: 80\n",
      "Experiment iteration: 81\n",
      "Experiment iteration: 82\n",
      "Experiment iteration: 83\n",
      "Experiment iteration: 84\n",
      "Experiment iteration: 85\n",
      "Experiment iteration: 86\n",
      "Experiment iteration: 87\n",
      "Experiment iteration: 88\n",
      "Experiment iteration: 89\n",
      "Experiment iteration: 90\n",
      "Experiment iteration: 91\n",
      "Experiment iteration: 92\n",
      "Experiment iteration: 93\n",
      "Experiment iteration: 94\n",
      "Experiment iteration: 95\n",
      "Experiment iteration: 96\n",
      "Experiment iteration: 97\n",
      "Experiment iteration: 98\n",
      "Experiment iteration: 99\n",
      "rfe_svm_scores:  [0.515625, 0.515625, 0.565625, 0.55625, 0.603125, 0.5375, 0.559375, 0.559375, 0.55, 0.55, 0.546875, 0.54375, 0.575, 0.55, 0.55, 0.55625, 0.59375, 0.58125, 0.553125, 0.578125, 0.49375, 0.528125, 0.559375, 0.559375, 0.58125, 0.546875, 0.51875, 0.540625, 0.546875, 0.56875, 0.5875, 0.578125, 0.578125, 0.559375, 0.53125, 0.553125, 0.53125, 0.54375, 0.55625, 0.58125, 0.5625, 0.5625, 0.5625, 0.553125, 0.55625, 0.540625, 0.53125, 0.575, 0.5875, 0.575, 0.509375, 0.571875, 0.525, 0.578125, 0.50625, 0.521875, 0.525, 0.56875, 0.55625, 0.55, 0.54375, 0.56875, 0.540625, 0.540625, 0.56875, 0.553125, 0.55, 0.56875, 0.58125, 0.5125, 0.571875, 0.58125, 0.559375, 0.5375, 0.578125, 0.56875, 0.590625, 0.6, 0.5875, 0.5125, 0.546875, 0.590625, 0.56875, 0.534375, 0.521875, 0.540625, 0.53125, 0.55, 0.559375, 0.575, 0.528125, 0.56875, 0.590625, 0.54375, 0.553125, 0.59375, 0.540625, 0.56875, 0.5625, 0.56875]\n",
      "\n",
      "\n",
      " rfe_lr_scores : [0.534375, 0.509375, 0.553125, 0.55, 0.60625, 0.540625, 0.540625, 0.55625, 0.546875, 0.55, 0.553125, 0.546875, 0.571875, 0.5125, 0.54375, 0.565625, 0.56875, 0.58125, 0.546875, 0.559375, 0.475, 0.528125, 0.546875, 0.534375, 0.56875, 0.559375, 0.515625, 0.559375, 0.546875, 0.5625, 0.58125, 0.565625, 0.578125, 0.571875, 0.53125, 0.5375, 0.546875, 0.553125, 0.534375, 0.5875, 0.596875, 0.546875, 0.59375, 0.54375, 0.509375, 0.559375, 0.51875, 0.553125, 0.55, 0.55, 0.503125, 0.565625, 0.51875, 0.578125, 0.5, 0.50625, 0.53125, 0.575, 0.578125, 0.515625, 0.53125, 0.55, 0.50625, 0.5125, 0.590625, 0.50625, 0.55625, 0.546875, 0.571875, 0.5125, 0.5625, 0.534375, 0.55, 0.5125, 0.575, 0.575, 0.559375, 0.6, 0.603125, 0.49375, 0.546875, 0.6, 0.584375, 0.546875, 0.525, 0.528125, 0.5375, 0.5375, 0.55625, 0.596875, 0.51875, 0.534375, 0.584375, 0.559375, 0.521875, 0.571875, 0.53125, 0.553125, 0.584375, 0.546875]\n",
      "\n",
      "\n",
      " no_rfe_scores :  [0.53125, 0.528125, 0.56875, 0.5875, 0.609375, 0.5375, 0.54375, 0.559375, 0.546875, 0.553125, 0.55625, 0.5375, 0.578125, 0.528125, 0.54375, 0.56875, 0.5875, 0.6, 0.553125, 0.584375, 0.496875, 0.54375, 0.5625, 0.54375, 0.584375, 0.571875, 0.509375, 0.559375, 0.565625, 0.56875, 0.60625, 0.565625, 0.56875, 0.5625, 0.5375, 0.5625, 0.55, 0.565625, 0.534375, 0.5875, 0.56875, 0.546875, 0.565625, 0.553125, 0.54375, 0.5875, 0.5375, 0.565625, 0.571875, 0.575, 0.5125, 0.575, 0.534375, 0.590625, 0.496875, 0.5375, 0.5375, 0.58125, 0.58125, 0.525, 0.553125, 0.559375, 0.53125, 0.528125, 0.603125, 0.534375, 0.55, 0.575, 0.590625, 0.515625, 0.590625, 0.55625, 0.5625, 0.515625, 0.58125, 0.55, 0.575, 0.60625, 0.58125, 0.51875, 0.540625, 0.584375, 0.56875, 0.5625, 0.515625, 0.55, 0.525, 0.534375, 0.559375, 0.5875, 0.525, 0.553125, 0.584375, 0.56875, 0.540625, 0.575, 0.540625, 0.565625, 0.578125, 0.559375]\n"
     ]
    }
   ],
   "source": [
    "def experiment(estimator, repeats, predictors_train, predictors_test, target_train, target_test, use_rfe=True):\n",
    "\n",
    "    if (use_rfe):        \n",
    "        selector = RFE(estimator, 3)\n",
    "        selector = selector.fit(predictors_train, target_train)\n",
    "        \n",
    "        predictors_train_selected = selector.transform(predictors_train)\n",
    "        predictors_test_selected = selector.transform(predictors_test)\n",
    "    else:\n",
    "        predictors_train_selected = predictors_train\n",
    "        predictors_test_selected = predictors_test\n",
    "\n",
    "    # score classifier\n",
    "    classifier = svm.SVC()\n",
    "    model = classifier.fit(predictors_train_selected, target_train)\n",
    "    model_score = model.score(predictors_test_selected, target_test)\n",
    "        \n",
    "    return model_score;\n",
    "    \n",
    "\n",
    "def whole_experiment(repeats):\n",
    "    rfe_svm_scores = []\n",
    "    rfe_lr_scores = []\n",
    "    no_rfe_scores = []\n",
    "\n",
    "    for i in range (0, repeats):\n",
    "        print(\"Experiment iteration:\", i)\n",
    "        \n",
    "        # stratified train_test_split\n",
    "        predictors_train, predictors_test, target_train, target_test = train_test_split(\n",
    "        predictors, target, test_size=0.20, shuffle=True, stratify=target)\n",
    "        \n",
    "        # normalise\n",
    "        predictors_train = min_max_scaler.fit_transform(predictors_train)\n",
    "        predictors_test = min_max_scaler.fit_transform(predictors_test)\n",
    "        \n",
    "        # SVM\n",
    "        estimatorSVM = svm.SVR(kernel=\"linear\")\n",
    "        rfe_svm_scores.append(experiment(estimatorSVM, repeats, predictors_train, predictors_test, target_train, target_test))\n",
    "        \n",
    "        # LR\n",
    "        estimatorLR = LogisticRegression()\n",
    "        rfe_lr_scores.append(experiment(estimatorLR, repeats, predictors_train, predictors_test, target_train, target_test))\n",
    "        \n",
    "        # Normal\n",
    "        no_rfe_scores.append(experiment(None, repeats, predictors_train, predictors_test, target_train, target_test, use_rfe=False))\n",
    "        \n",
    "    return rfe_svm_scores, rfe_lr_scores, no_rfe_scores\n",
    "\n",
    "# Run the whole experiment\n",
    "rfe_svm_scores, rfe_lr_scores, no_rfe_scores = whole_experiment(100)\n",
    "print(\"rfe_svm_scores: \", rfe_svm_scores)\n",
    "print(\"\\n\\n rfe_lr_scores :\", rfe_lr_scores)\n",
    "print(\"\\n\\n no_rfe_scores : \", no_rfe_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RFE SVM</th>\n",
       "      <th>RFE LR</th>\n",
       "      <th>No RFE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.555563</td>\n",
       "      <td>0.549313</td>\n",
       "      <td>0.557344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.022945</td>\n",
       "      <td>0.027281</td>\n",
       "      <td>0.024815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.493750</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>0.496875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.540625</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.556250</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.559375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.571875</td>\n",
       "      <td>0.568750</td>\n",
       "      <td>0.575000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.603125</td>\n",
       "      <td>0.606250</td>\n",
       "      <td>0.609375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          RFE SVM      RFE LR      No RFE\n",
       "count  100.000000  100.000000  100.000000\n",
       "mean     0.555563    0.549313    0.557344\n",
       "std      0.022945    0.027281    0.024815\n",
       "min      0.493750    0.475000    0.496875\n",
       "25%      0.540625    0.531250    0.537500\n",
       "50%      0.556250    0.550000    0.559375\n",
       "75%      0.571875    0.568750    0.575000\n",
       "max      0.603125    0.606250    0.609375"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.DataFrame({'RFE SVM':rfe_svm_scores, 'RFE LR': rfe_lr_scores, 'No RFE':no_rfe_scores })\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0.98,'')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAHf5JREFUeJzt3XucHWWB5vHfY0IQAREH7YEQCCOoQbxBCzoiNqiIH3eCDiqgq2QHiKgxDt4mWXaBRaN4W5wZcTRMkOAlRF0vjcQgOLSIiiZoBEMIRAQTQG5RIRECic/+UdVQHLq7Trq76Eue7+dzPpyqet+qt85L+jn11qkq2SYiImIgTxrpBkRExOiXsIiIiFoJi4iIqJWwiIiIWgmLiIiolbCIiIhaCYuIfkiaIemqkW5HlaRXSFo90u2IbU/CIkYFSYdK+qmkP0taL+knkl4y0u0aaZIsad/eads/tv2chrZ1gaSPNrHuGPsmjnQDIiQ9Ffge8C7g68Ak4BXApmHezgTbW4ZznfGofL7jW44sYjR4NoDtRba32H7A9g9sX9tbQNLJklZJul/S9ZIOLOdPk9Qj6U+SVkqaXqlzgaT/kLRE0kbgcEnbS/q0pN9LulPSFyTtMEDbJOnfyyOeGyS9qpz5ZknXtBT8gKTv9LOSXSQtkHSHpNskfVTShHLZvpJ+VG7jHkmLy/lXltV/LWmDpGMldUlaV1nvLZI+JOlaSRvLbXRI+n75WV0uaddK+W9I+kO5rSslPa+cPxN4G/DhclsXD+bzHeBzjLHOdl55jegLeCpwL7AQeB2wa8vyNwO3AS8BBOwL7A1sB6wB/ifF0cgRwP3Ac8p6FwB/Bl5O8cXoycBngW7g6cDOwMXAx/tp1wxgM3Bqua1jy/U9HdgeWA9Mq5T/FXBMP+v6DvBFYEfgmcAvgHeWyxYBp1XaeGilnoF9K9NdwLrK9C3A1UAHMBm4C/gl8OKyjf8FnFEp/0/lfm9ffhYrKssuAD5amd7qz3ek/1/Kq7lXjixixNm+DziU4g/jecDdkroldZRFTgI+aXuZC2ts3wq8FNgJONv2Q7b/i2I46/jK6r9r+ye2/0oxrHUycKrt9bbvBz4GHDdA8+4CPmv7YduLgdXA621vAhYD/x2g/IY+tdz+Y5T78Trgn21vtH0XcE5luw9ThN8eth+0vbUn1f/d9p22bwN+DPzc9q/KNn6bIjgAsH2+7fvLZWcCL5S0Sz/r3arP1/aDW9nuGEMSFjEq2F5le4btPYEDgD0ovvkCTAF+20e1PYC1ZRD0upXiG3avtZX3zwCeAlxTDqv8CVhazu/Pbbard9u8tdwuFEdCb5Uk4O3A18s/wq16j4LuqGz3ixRHGAAfpjhi+kU51PNPA7SnL3dW3j/Qx/ROUJxTkHS2pN9Kuo/iqARgt37Wu7Wfb4xjOcEdo47tGyRdALyznLUWeFYfRW8Hpkh6UuUP2l7AjdXVVd7fQ/HH83nlt/B2TJakSmDsRTGMhe2rJT1EcTL+reWrL2spjmp2s725daHtP1Ac8SDpUOBySVfaXtNmG9v1VuBo4NUUQbEL8EeKoILHflaw9Z9vjGM5sogRJ+m55cnhPcvpKRRDHVeXRf4T+KCkg1TYV9LewM+BjRQnZbeT1AX8A3BRX9sp/+CdB5wj6ZnltiZLeu0AzXsmMLtc/5uBacCSyvILgc8Bm/sbPrJ9B/AD4DOSnirpSZKeJemVZRve3LvvFH+8DfT+quhO4O8GaN/W2JkitO6lOML6WMvy1m1t1ecb41vCIkaD+4FDgJ+Xv6q5GvgN8AEA298A5gFfK8t+B3i67YeA6RTnA+4BPg+8w/YNA2zrXyhO2l5dDsVcDgx03cLPgf3K9c8D3mT73sryL1MMm325Zh/fQXGS+HqKQPgmsHu57CXlvm+gOGp5n+3flcvOBBaWw1dvqdlGnQsphpFuK9txdcvyBcD+5ba+M8jPN8YpPXY4NiK2Rvmz27uAA23fNNLtiWhKjiwihuZdwLIERYx3OcEdMUiSbqE4OfyGEW5KROMyDBUREbUyDBUREbXGzTDUbrvt5qlTp450MxqzceNGdtxxx5FuRgxS+m/sGu99d80119xje6ALU4FxFBZTp05l+fLlI92MxvT09NDV1TXSzYhBSv+NXeO97yTd2k65DENFRESthEVERNRKWERERK2ERURE1EpYRERErYRFRETUSlhERESthEVERNQaNxflRUQMRvFU3KEb7/fZy5FFRGzTbA/42vtfvldbZrwHBSQsIiKiDQmLiIiolbCIiIhaOcEdMUQ5QRrbghxZRAxROyc/2zlJGjGaJSwiIqJWwiIiImrlnMUokXHviBjNcmQxSuTCoIgYzRoNC0lHSVotaY2kOf2UeYuk6yWtlPS1yvwTJN1Uvk5osp0RETGwxoahJE0AzgVeA6wDlknqtn19pcx+wFzg5bb/KOmZ5fynA2cAnYCBa8q6f2yqvRER0b8mjywOBtbYvtn2Q8BFwNEtZU4Gzu0NAdt3lfNfC1xme3257DLgqAbbGhERA2jyBPdkYG1leh1wSEuZZwNI+gkwATjT9tJ+6k5u3YCkmcBMgI6ODnp6eoar7aPSeN+/8S79N3al75oNi75+3tN6BnYisB/QBewJ/FjSAW3WxfZ8YD5AZ2enu7q6htDcUW7pJYzr/Rvv0n9jV/oOaHYYah0wpTK9J3B7H2W+a/th278DVlOERzt1IyLiCdJkWCwD9pO0j6RJwHFAd0uZ7wCHA0jajWJY6mbgUuBISbtK2hU4spwXEREjoLFhKNubJc2i+CM/ATjf9kpJZwHLbXfzaChcD2wBPmT7XgBJH6EIHICzbK9vqq0RETGwRq/gtr0EWNIy7/TKewPvL1+tdc8Hzm+yfRER0Z5cwR0REbUSFhERUSthERERtRIWERFRK2ERERG1EhYREVErYREREbUSFhERUSthERERtRIWERFRK2ERERG1EhYREVErYREREbUSFhERUSthERERtRIWERFRK2ERERG1EhYREVErYREREbUSFhERUavRsJB0lKTVktZImtPH8hmS7pa0onydVFn2SUkrJa2S9G+S1GRbIyKifxObWrGkCcC5wGuAdcAySd22r28putj2rJa6fw+8HHhBOesq4JVAT1PtjYiI/jV5ZHEwsMb2zbYfAi4Cjm6zroEnA5OA7YHtgDsbaWVERNRq7MgCmAysrUyvAw7po9wxkg4DbgROtb3W9s8kXQHcAQj4nO1VrRUlzQRmAnR0dNDT0zPMuzC6jPf9G+/Sf2NX+q7ZsOjrHINbpi8GFtneJOkUYCFwhKR9gWnAnmW5yyQdZvvKx6zMng/MB+js7HRXV9dwtn90WXoJ43r/xrv039iVvgOaHYZaB0ypTO8J3F4tYPte25vKyfOAg8r3bwSutr3B9gbg+8BLG2xrREQMoMmwWAbsJ2kfSZOA44DuagFJu1cmpwO9Q02/B14paaKk7ShObj9uGCoiIp4YjQ1D2d4saRZwKTABON/2SklnActtdwOzJU0HNgPrgRll9W8CRwDXUQxdLbV9cVNtjYiIgTV5zgLbS4AlLfNOr7yfC8zto94W4J1Nti0iItqXK7gjIqJWwiIiImo1OgwVETHSXvh/fsCfH3h4SOuYOueSIdXfZYft+PUZRw5pHSMtYRER49qfH3iYW85+/aDr9/T0DPk6i6GGzWiQYaiIiKiVsIiIiFoZhnqCZNw0IsayhMUTJOOmETGWZRgqIiJqJSwiIqJWwiIiImrlnEVEjeH4cQLkBwoxtiUsImoM9ccJkB8oxNiXYaiIiKiVsIiIiFoJi4iIqJWwiIiIWgmLiIiolbCIiIhaCYuIiKiVsIiIiFqNhoWkoyStlrRG0pw+ls+QdLekFeXrpMqyvST9QNIqSddLmtpkWyMion+NXcEtaQJwLvAaYB2wTFK37etbii62PauPVVwIzLN9maSdgL821daIiBhYk0cWBwNrbN9s+yHgIuDodipK2h+YaPsyANsbbP+luaZGRMRAmrw31GRgbWV6HXBIH+WOkXQYcCNwqu21wLOBP0n6FrAPcDkwx/aWakVJM4GZAB0dHfT09Az7TgynobRvw4YNw7J/o/0zGq2G+rml/0ZW/u0NXZNhoT7muWX6YmCR7U2STgEWAkeU7XoF8GLg98BiYAaw4DErs+cD8wE6Ozs91Bu1NWrpJUO6kdxw3IhuqG3YZg3D55b+Gzk73/p83nvrEFdy7xDbMA26uq4bYiNGVpNhsQ6YUpneE7i9WsB2tQvOAz5Rqfsr2zcDSPoO8FJawiIios79q87OI42HQZPnLJYB+0naR9Ik4Digu1pA0u6VyenAqkrdXSU9o5w+Amg9MR4REU+Qxo4sbG+WNAu4FJgAnG97paSzgOW2u4HZkqYDm4H1FENN2N4i6YPADyUJuIbiyCMiIkZAow8/sr0EWNIy7/TK+7nA3H7qXga8oMn2PZF2njaH5y983KUmW2fhUNsAMLSH+ETEtilPynuCZNw0Isay3O4jIiJqJSwiIqJWwiIiImolLCIiolbCIiIiaiUsIiKiVsIiIiJqJSwiIqJW2xflSToU2M/2l8p7Nu1k+3fNNS1idBiWq+8hV+DHmNZWWEg6A+gEngN8CdgO+Arw8uaaFjE6DPXqe8gV+DH2tTsM9UaKu8JuBLB9O7BzU42KiIjRpd2weMi2KR9eJGnH5poUERGjTbth8XVJXwSeJulkisec5pbhERHbiLbOWdj+tKTXAPdRnLc4vbyFeEREbANqw0LSBOBS268GEhAREdug2mEo21uAv0ja5QloT0REjELtXmfxIHCdpMsofxEFYHt2I62KiIhRpd2wuKR8RUTENqjdE9wLJU0Cnl3OWm374eaaFRERo0lbP52V1AXcBJwLfB64UdJhbdQ7StJqSWskPe5+CZJmSLpb0orydVLL8qdKuk3S59ram4iIaES7w1CfAY60vRpA0rOBRcBB/VUof0V1LvAaYB2wTFK37etbii62Pauf1XwE+FGbbYyIiIa0Gxbb9QYFgO0bJW1XU+dgYI3tmwEkXQQcDbSGRZ8kHQR0AEsp7ks15g353j5Lh1Z/lx3quiwiom/thsVySQuAL5fTbwOuqakzGVhbmV4HHNJHuWPKIa0bgVNtr5X0JIqjmbcDr+pvA5JmAjMBOjo66OnpaWNXRsYFRw3tDikzlm4c8jqAUf0ZjWZD/dw2bNgwLJ99+m9whvK5pe8K7YbFu4D3ALMBAVdSnLsYiPqY55bpi4FFtjdJOoXiJs5HAO8GlpTB0e8GbM8H5gN0dnZ6qHf1HNWWXjLku5bGIA3DZz8cd53N/wODNMTPLX1XaDcsJgL/avv/wiPnI7avqbMOmFKZ3hO4vVrA9r2VyfOAT5TvXwa8QtK7gZ2ASZI22B6GhwpERMTWavdGgj8EdqhM70BxM8GBLAP2k7RP+bPb44DuagFJu1cmpwOrAGy/zfZetqcCHwQuTFBERIycdo8snmx7Q++E7Q2SnjJQBdubJc0CLgUmAOfbXinpLGC57W5gtqTpwGZgPTBjMDsRETGQ/Lhk6NoNi42SDrT9SwBJncADdZVsLwGWtMw7vfJ+LjC3Zh0XABe02c6IiMcY6lMOp865ZMjrGA/aDYv3Ad+QdDvFSeo9gGMba1VERIwq7YbFPsCLgb0oHrH6Uh7/y6aIiBin2j3B/b9t3wc8jeKK7PnAfzTWqoiIGFXaDYst5X9fD3zB9neBSc00KSIiRpt2w+K28hncbwGWSNp+K+pGRMQY1+4f/LdQ/AT2KNt/Ap4OfKixVkVExKjS7vMs/gJ8qzJ9B3BHU42KiIjRpd1fQ0Vs04Z8URfkwq4Y0xIWETWG44KsXNgVY11OUkdERK2ERURE1EpYRERErYRFRETUSlhERESthEVERNRKWERERK2ERURE1EpYRERErYRFRETUSlhERESthEVERNRqNCwkHSVptaQ1kub0sXyGpLslrShfJ5XzXyTpZ5JWSrpW0rFNtjMiIgbW2F1nJU0AzqV4Zvc6YJmkbtvXtxRdbHtWy7y/AO+wfZOkPYBrJF1aPngpIiKeYE0eWRwMrLF9s+2HgIuAo9upaPtG2zeV728H7gKe0VhLIyJiQE0+z2IysLYyvQ44pI9yx0g6DLgRONV2tQ6SDgYmAb9trShpJjAToKOjg56enuFp+Sg13vdvvEv/jV3pu2bDQn3Mc8v0xcAi25sknQIsBI54ZAXS7sCXgRNs//VxK7PnA/MBOjs73dXVNUxNH4WWXsK43r/xLv03dqXvgGaHodYBUyrTewK3VwvYvtf2pnLyPOCg3mWSngpcAvwv21c32M6IiKjRZFgsA/aTtI+kScBxQHe1QHnk0Gs6sKqcPwn4NnCh7W802MaIiGhDY8NQtjdLmgVcCkwAzre9UtJZwHLb3cBsSdOBzcB6YEZZ/S3AYcDfSOqdN8P2iqbaGxER/WvynAW2lwBLWuadXnk/F5jbR72vAF9psm0REdG+XMEdERG1EhYREVErYREREbUSFhERUSthERERtRIWERFRK2ERERG1EhYREVErYREREbUSFhERUSthERERtRIWERFRK2ERERG1EhYREVErYREREbUSFhERUSthERERtRIWERFRK2ERERG1EhYREVGr0bCQdJSk1ZLWSJrTx/IZku6WtKJ8nVRZdoKkm8rXCU22MyIiBjaxqRVLmgCcC7wGWAcsk9Rt+/qWoottz2qp+3TgDKATMHBNWfePTbU3IiL61+SRxcHAGts3234IuAg4us26rwUus72+DIjLgKMaamdERNRo7MgCmAysrUyvAw7po9wxkg4DbgROtb22n7qTWytKmgnMBOjo6KCnp2d4Wj5Kjff9G+/Sf2NX+q7ZsFAf89wyfTGwyPYmSacAC4Ej2qyL7fnAfIDOzk53dXUNqcGj2tJLGNf7N96l/8au9B3Q7DDUOmBKZXpP4PZqAdv32t5UTp4HHNRu3YiIeOI0GRbLgP0k7SNpEnAc0F0tIGn3yuR0YFX5/lLgSEm7StoVOLKcFxERI6CxYSjbmyXNovgjPwE43/ZKSWcBy213A7MlTQc2A+uBGWXd9ZI+QhE4AGfZXt9UWyMiYmBNnrPA9hJgScu80yvv5wJz+6l7PnB+k+2LiJD6OkXaUuYT9euxH3dadVzJFdwRsU2zPeDriiuuqC0z3oMCEhYREdGGhEVERNRq9JxFtC/jpmNXO30H9f2XvovRLEcWo0TGTceudvqlnf6LGM0SFhERUSthERERtRIWERFRK2ERERG1EhYREVErYREREbUSFhERUSthERERtRIWERFRK2ERERG1EhYREVErYREREbUSFhERUSthERERtRIWERFRq9GwkHSUpNWS1kiaM0C5N0mypM5yejtJCyVdJ2mVpLlNtjMiIgbWWFhImgCcC7wO2B84XtL+fZTbGZgN/Lwy+83A9rafDxwEvFPS1KbaGhERA2vyyOJgYI3tm20/BFwEHN1HuY8AnwQerMwzsKOkicAOwEPAfQ22NSIiBtBkWEwG1lam15XzHiHpxcAU299rqftNYCNwB/B74NO21zfY1oiIGMDEBtfd11PsH3nQsKQnAecAM/oodzCwBdgD2BX4saTLbd/8mA1IM4GZAB0dHfT09AxLw0ejDRs2jOv9G+/Sf2NX+q7QZFisA6ZUpvcEbq9M7wwcAPRIAvhboFvSdOCtwFLbDwN3SfoJ0Ak8JixszwfmA3R2drqrq6uZPRkFenp6GM/7N96l/8au9F2hyWGoZcB+kvaRNAk4DujuXWj7z7Z3sz3V9lTgamC67eUUQ09HqLAj8FLghgbbGhERA2gsLGxvBmYBlwKrgK/bXinprPLoYSDnAjsBv6EInS/ZvraptkZExMAavc7C9hLbz7b9LNvzynmn2+7uo2xXeVSB7Q2232z7ebb3t/2pJtsZ0ZRFixZxwAEH8KpXvYoDDjiARYsWjXSTIgalyXMWEdu0RYsWcdppp7FgwQK2bNnChAkTOPHEEwE4/vjjR7h1EVsnt/uIaMi8efNYsGABhx9+OBMnTuTwww9nwYIFzJs3b6SbFrHVEhYRDVm1ahWHHnroY+YdeuihrFq1aoRaFDF4CYuIhkybNo2rrrrqMfOuuuoqpk2bNkItihi8hEVEQ0477TROPPFErrjiCjZv3swVV1zBiSeeyGmnnTbSTYvYajnBHdGQ3pPY733ve1m1ahXTpk1j3rx5ObkdY1LCIqJBxx9/PMcff3yuAo4xL8NQERFRK2ERERG1EhYREVErYREREbUSFhERUUu260uNAZLuBm4d6XY0aDfgnpFuRAxa+m/sGu99t7ftZ9QVGjdhMd5JWm67c6TbEYOT/hu70neFDENFRESthEVERNRKWIwd80e6ATEk6b+xK31HzllEREQbcmQRERG1EhYREVErYdEASVskrZD0G0kXS3paOX+qpAfKZb2vSZJmSLq7Zf7+faz3NEkrJV1bljlE0pmSPt5S7kWSVpXvb5H045blKyT9psnPYLxpsE839DHvTEm3lXWul5R7mg+BJEv6TGX6g5LO3Ir61b68QdKplWXVvloh6exyfo+k1ZX53xzWnRoBuUV5Mx6w/SIASQuB9wC9D17+be+yXpIAFtue1d8KJb0M+G/AgbY3SdoNmAQsAr4PzK0UPw74WmV6Z0lTbK+VlMe0Dc6w92mNc2x/WtJ+wDWSvmn74UGua1u3CfhHSR+3PdiL6xbbniXpb4DVZX+sLZedY/vTfdR5m+3lg9zeqJMji+b9DJg8DOvZHbjH9iYA2/fYvt32auBPkg6plH0LcFFl+uvAseX74ykCJgZvuPq0lu2bgL8Auz4R2xunNlP8ounU1gWS9pb0w/Jo/YeS9hpoRbbvBdZQ/HvcpiQsGiRpAvAqoLsy+1mVQ9NzK/OPbRmy2KFldT8Apki6UdLnJb2ysmwRxdEEkl4K3Fv+ken1TeAfy/f/AFw8DLu3TRrmPm1newcCN9m+a4hN39adC7xN0i4t8z8HXGj7BcBXgX8baCVlmDwZuLYy+9RKH7+2Mv+rlfmfGoZ9GFEZhmrGDpJWAFOBa4DLKsseN2RRGnDIwvYGSQcBrwAOBxZLmmP7AoqjiJ9K+gBFaLQeOawH/ijpOGAVxTfV2DrD3qc1TpV0MvB3wFGDXEeUbN8n6UJgNvBAZdHLePSL1JeBT/azimMlHQ48BzjZ9oOVZRmGikHrHd/em+K8wnuGY6W2t9jusX0GMAs4ppy/FrgFeGU57+t9VF9M8e0qQ1CD00ifDuAc28+hGD68UNKTG97etuCzwInAjgOU6e/Cs8W2n0fxZe0zkv52uBs32iUsGmT7zxTfZD4oabuhrEvSc8qTnb1exGPvsrsIOIfiW+66PlbxbYpvTZcOpR3buuHs0za39y1gOXBC09sa72yvp/gidWJl9k8ph3CBtwFX1azjZxRHIO9roo2jWcKiYbZ/BfyaR/+H7E/r+PbftyzfCVhY/pTyWmB/4MzK8m8Az+OxJ7ar7bjf9idsPzSoHYlHDGOfAjxF0rrK6/19lDkLeL+k/Hsdus9Q3HK812zgf5T/pt5OeyHwibLOzjXlqucsLh9cc0eP3O4jIiJq5ZtKRETUSlhERESthEVERNRKWERERK2ERURE1EpYRJQqd5btfc0ZoOwbqneRlXSWpFcPQxueJundQ11PxHDLT2cjSpI22N6pzbIXAN+zPay3npY0tVzvAVtRZ4LtLcPZjohWObKIqCHp7N6LISV9ury4bjrwqfII5FmSLpD0prL8LZI+JulnkpZLOlDSpZJ+K+mUssxO5V1OfynpOklHl5s7m0dvTPgpFT6l4jka10k6tqzfJekKSV8DrhuBjyW2MbmRYMSjem8W2OvjFDcMfCPwXNuW9DTbf5LUTeXIQsXzK6rW2n6ZpHOAC4CXU9ytdCXwBeBB4I3lDe52A64u1zkHOKDy7IxjKG7t8kKKK4+XSbqy3MbBZdnfDe/HEPF4CYuIRz3ygKNekiZS/GH/T0mXAN9rc129tzC/DtjJ9v3A/ZIeVPGUvY3AxyQdBvyV4vkYHX2s51BgUTnMdKekHwEvAe4DfpGgiCdKhqEiBmB7M8U3+P8HvAFY2mbVTeV//1p53zs9keKmdc8ADioD6k6KI49WjztkqdjYZlsihixhETEASTsBu9heAvwzxZAQwP1A3Y3kBrILcJfth8vnJOzdz3qvpLgh4QRJzwAOA34xhO1GDEqGoSIe1XrOYinwr8B3y+dJiEcfzXkRcJ6k2cCbBrGtrwIXS1oOrABugOKxnZJ+Iuk3FM9W/zDFA3p+TfGshQ/b/oOk5w5imxGDlp/ORkRErQxDRURErYRFRETUSlhERESthEVERNRKWERERK2ERURE1EpYRERErf8P2678LGkN3mMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.boxplot()\n",
    "plt.title(\"Score by estimator\")\n",
    "plt.xlabel('Estimator')\n",
    "plt.ylabel('score')\n",
    "\n",
    "plt.suptitle(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**TODO**: <BR>  1) Add 150 words summary <BR>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mean score and associated variance is nearly the same for the 3 models.\n",
    "\n",
    "The **best mean score comes from RFE on Logitic Regression**, followed closely by the model with no RFE and the model using RFE on SVM.\n",
    "\n",
    "This shows that we don't need all the features in order to get the same accuracy as if we had the whole data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "  * Build two pipelines for training classifiers for the dataset winequality_red.csv and summarise the results from training classifiers with the pipelines in a markdown cell (max 150 words).\n",
    "  * The two pipelines should include feature-selection methods, and the feature-selection method in pipeline 1 should be different from the feature-selection method in pipeline 2.\n",
    "\n",
    "#### Resources:\n",
    "  * https://machinelearningmastery.com/automate-machine-learning-workflows-pipelines-python-scikit-learn/\n",
    "  * https://towardsdatascience.com/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976\n",
    "  * https://ramhiser.com/post/2018-03-25-feature-selection-with-scikit-learn-pipeline/\n",
    "  \n",
    "#### Rubric\n",
    "\n",
    "| Beginning [0-8]       | Developing [9-12]           | Accomplished [13-16]  |Exemplary [17-20] |\n",
    "| ------------- |:-------------:| -----:|-------------|\n",
    "| Pipelines attempted but with run-time errors.    | One pipeline implemented without errors. Results not summarised. | Two pipelines implemented without errors but both using the same feature-selection method. Results summarised. | Two pipelines implemented without errors. The feature-selection method in pipeline 1 different from the feature-selection method in pipeline 2. Results summarised. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5084394654088051\n"
     ]
    }
   ],
   "source": [
    "# create feature union\n",
    "features = []\n",
    "features.append(('rfe', RFE(LogisticRegression(), 3)))\n",
    "feature_union = FeatureUnion(features)\n",
    "\n",
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(('feature_union', feature_union))\n",
    "estimators.append(('logistic', LogisticRegression()))\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "# evaluate pipeline\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(model, predictors, target, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5728577044025157\n"
     ]
    }
   ],
   "source": [
    "# create feature union\n",
    "features = []\n",
    "features.append(('pca', PCA(n_components=3)))\n",
    "features.append(('select_best', SelectKBest(k=6)))\n",
    "feature_union = FeatureUnion(features)\n",
    "\n",
    "# create pipeline\n",
    "estimators = []\n",
    "estimators.append(('feature_union', feature_union))\n",
    "estimators.append(('logistic', LogisticRegression()))\n",
    "model = Pipeline(estimators)\n",
    "\n",
    "# evaluate pipeline\n",
    "seed = 7\n",
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(model, predictors, target, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**TODO**: <BR>  1) Add 150 words summary <BR>  2) Ensure Feature Selection is done in both pipelines.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * We can see that the pipeline using PCA is produces a model mean accuracy superior to the the pipeline using RFE. (57% pca vs 50% for RFE)\n",
    "  * Both PCA and RFE are using 3 features only. This shows that the features computed by PCA are more representative of the data set that the features selected by RFE.\n",
    "\n",
    "PCA is a dimensionality reduction technique, which when used within a pipeline needs to be followed by the SelectKBest() function in order to select the features according to the k highest scores.\n",
    "\n",
    "**I FORGOT TO NORMALISE THE DATA!!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
