{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student Details\n",
    "\n",
    "Name: Fergus Mc Hale\n",
    "\n",
    "ID: 18203485"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may add two extra libraries: one to make a train-test split and one to perform a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_ATTRIBUTE = '2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4  (CE6002)\n",
    "* Perform classification on dataset_1 using an SVM with linear kernel and experiment with the C-parameter to find the widest margin solution with a hard margin and a soft margin.\n",
    "* This task provides an insight in how SVMâ€™s try to find a hyperplane (which in two dimensions is a line) which divides two classes with the maximum margin on either side of the hyperplane. You can use the C-parameter as a form of regularization; with this parameter you can allow the SVM to miss-classify certain points to allow a wider margin and thus, hopefully, a better performance out-of-sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1 = pd.read_csv(\"./dataset_1.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = dataset_1[TARGET_ATTRIBUTE].values\n",
    "\n",
    "X1 = dataset_1.drop(TARGET_ATTRIBUTE, axis=1).values\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2 = pd.read_csv(\"./dataset_2.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = dataset_2[TARGET_ATTRIBUTE].values\n",
    "\n",
    "# predictor attributes\n",
    "X2 = dataset_2.drop(TARGET_ATTRIBUTE, axis=1).values\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataSet 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_3 = pd.read_csv(\"./dataset_3.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y3 = dataset_3[TARGET_ATTRIBUTE].values\n",
    "\n",
    "# predictor attributes\n",
    "X3 = dataset_3.drop(TARGET_ATTRIBUTE, axis=1).values\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifer_c01 = GridSearchCV(svm.SVC(), {'kernel': ['linear'], 'C': [.01]}, scoring='accuracy')\n",
    "classifer_c1 = GridSearchCV(svm.SVC(), {'kernel': ['linear'], 'C': [1]}, scoring='accuracy')\n",
    "classifer_c10 = GridSearchCV(svm.SVC(), {'kernel': ['linear'], 'C': [10]}, scoring='accuracy')\n",
    "classifer_c100 = GridSearchCV(svm.SVC(), {'kernel': ['linear'], 'C': [100]}, scoring='accuracy')\n",
    "classifer_c1000 = GridSearchCV(svm.SVC(), {'kernel': ['linear'], 'C': [1000]}, scoring='accuracy')\n",
    "\n",
    "classifer_c01.fit(X1_train, y1_train)\n",
    "classifer_c1.fit(X1_train, y1_train)\n",
    "classifer_c10.fit(X1_train, y1_train)\n",
    "classifer_c100.fit(X1_train, y1_train)\n",
    "classifer_c1000.fit(X1_train, y1_train)\n",
    "\n",
    "classifiers = [(classifer_c01, 'C = .01'),\n",
    "               (classifer_c1, 'C = 1'),\n",
    "               (classifer_c10, 'C = 10'),\n",
    "               (classifer_c100, 'C = 100'),\n",
    "               (classifer_c1000, 'C = 1,000')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eout(support_vector_size,number_of_training_samples):\n",
    "    return support_vector_size/(number_of_training_samples -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classifier(clf, X_train,y_train,title=None):\n",
    "    # plot the decision function\n",
    "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=30, cmap=plt.cm.Paired)\n",
    "    plt.title(title)\n",
    "    # plot the decision function\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect(aspect=1)\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    # create grid to evaluate model\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    Z = clf.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "    # plot support vectors\n",
    "    ax.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=100,  linewidth=1, facecolors='none', edgecolors='k')\n",
    "    print('Total Support Vectors {0}'.format(len(clf.support_vectors_)))\n",
    "    print('Training Samples {0}'.format(len(X_train)))\n",
    "    print(clf.support_vectors_)\n",
    "    expected_eout = get_eout(len(clf.support_vectors_),len(X_train))\n",
    "    print('E-out {0}'.format(expected_eout))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier in classifiers:\n",
    "    plot_classifier(classifier[0].best_estimator_, X1_train, y1_train,classifier[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for classifier in classifiers:\n",
    "    plot_classifier(classifier[0].best_estimator_, X1_test, y1_test,classifier[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5  (CE6002)\n",
    "* Explore the provided data sets (dataset_1, dataset_2 and dataset_3) using Support Vector Machines and choose suitable kernels and hyper-parameters.\n",
    "* This task provides an insight in how a kernel can allow the SVM (a linear classifier) to be used on non-linearly separable data sets by casting the data in some higher dimensional space as determined by the kernel you choose. In addition to choosing a suitable kernel, you will find that tuning the hyper-parameters of the SVM is important. Take some time to explore various kernels and values of the hyper-parameters to get a feel for how they affect performance and then use a structured approach to arrive at your final conclusions. Take into consideration the out-of-sample error (simulated and based on theory (see lecture 14 from Learning from Data)). \n",
    "* For this task you may import two extra libraries: one to create train-test splits and one to perform a grid search of your choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_svm(X_train,y_train):\n",
    "    parameters = [{'kernel': ['linear','rbf','poly','sigmoid'],'C': [1, 10, 100], 'gamma': [0.001, 0.0001,1,10,100,1000,\"auto\"]}]\n",
    "\n",
    "    clf = GridSearchCV(svm.SVC(), parameters, scoring='accuracy', cv=10, n_jobs=-1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ds1_train = find_optimal_svm(X1_train,y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_classifier2(clf, X_train, y_train):\n",
    "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, s=30, cmap=plt.cm.Paired)\n",
    "    plt.title(str(clf.best_params_))\n",
    "    # plot the decision function\n",
    "    ax = plt.gca()\n",
    "    ax.set_aspect(aspect=1)\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    # create grid to evaluate model\n",
    "    xx = np.linspace(xlim[0], xlim[1], 30)\n",
    "    yy = np.linspace(ylim[0], ylim[1], 30)\n",
    "    YY, XX = np.meshgrid(yy, xx)\n",
    "    xy = np.vstack([XX.ravel(), YY.ravel()]).T\n",
    "    Z = clf.best_estimator_.decision_function(xy).reshape(XX.shape)\n",
    "\n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(XX, YY, Z, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "    # plot support vectors\n",
    "    ax.scatter(clf.best_estimator_.support_vectors_[:, 0], clf.best_estimator_.support_vectors_[:, 1], s=100,  linewidth=1, facecolors='none', edgecolors='k')\n",
    "    print('Total Support Vectors {0}'.format(len(clf.best_estimator_.support_vectors_)))\n",
    "    print('Training Samples {0}'.format(len(X1_train)))\n",
    "    print(clf.best_estimator_.support_vectors_)\n",
    "    expected_eout = get_eout(len(clf.best_estimator_.support_vectors_),len(X1_train))\n",
    "    print('E-out {0}'.format(expected_eout))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier2(clf_ds1_train,X1_train,y1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ds1_test = find_optimal_svm(X1_test,y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier2(clf_ds1_test,X1_test,y1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ds2_train = find_optimal_svm(X2_train,y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier2(clf_ds2_train,X2_train,y2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ds2_test = find_optimal_svm(X2_test,y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier2(clf_ds2_test,X2_test,y2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ds3_train = find_optimal_svm(X3_train,y3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier2(clf_ds3_train,X3_train,y3_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_ds3_test = find_optimal_svm(X3_test,y3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_classifier2(clf_ds3_test,X3_test,y3_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
